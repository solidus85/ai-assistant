version: '3.8'

services:
  web:
    build: .
    container_name: ai-assistant
    ports:
      - "5000:5000"
    environment:
      - HOST=0.0.0.0
      - PORT=5000
      - DEBUG=False
      # Use host.docker.internal to connect to Ollama running on Windows host
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    env_file:
      - .env
    volumes:
      # Mount secret key as read-only
      - ./.secret_key:/app/.secret_key:ro
      # For development: mount source code for hot-reload (optional)
      # - ./src:/app/src
      # - ./config.py:/app/config.py
      # - ./run.py:/app/run.py
    restart: unless-stopped
    extra_hosts:
      # Ensures host.docker.internal works on Linux hosts too
      - "host.docker.internal:host-gateway"