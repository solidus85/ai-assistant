# Context Window Size Impact on Speed

## Quick Reference

| Context Size | Speed Impact | Best For |
|-------------|--------------|----------|
| 4K (4,096) | Fastest ‚ö°‚ö°‚ö° | Quick queries, single questions |
| **8K (8,192)** | Fast ‚ö°‚ö° | **Most conversations (current)** |
| 16K (16,384) | Moderate ‚ö° | Long conversations, code review |
| 32K (32,768) | Slower üêå | Very long documents |

## Speed Improvements by Context Size

### For Llama 3.1 8B:
- **32K ‚Üí 16K**: ~25% faster
- **16K ‚Üí 8K**: ~20-30% faster ‚úÖ (your current change)
- **8K ‚Üí 4K**: ~15-20% faster

### Expected Response Times with 8K Context:
- **Llama 3.1**: ~2 seconds (was 2-3s with 16K)
- **Mistral**: ~1-1.5 seconds
- **Mixtral**: ~15-20 seconds (was 25-30s)

## Trade-offs

### 8K Context (Current Setting):
**Pros:**
- ‚úÖ 20-30% faster responses
- ‚úÖ Still enough for most conversations
- ‚úÖ Good for coding questions
- ‚úÖ Handles ~10-15 messages of history

**Cons:**
- ‚ùå May truncate very long code files
- ‚ùå Less conversation history
- ‚ùå Might lose context in long chats

### When to Use Different Sizes:

**4K Context** - Use when:
- You need maximum speed
- Single question/answer
- No conversation history needed
```bash
python utilities/speed_profiles.py fast
```

**8K Context** - Use when:
- Daily conversations
- Moderate code snippets
- Balance of speed/context
```bash
python utilities/speed_profiles.py balanced
```

**16K Context** - Use when:
- Reviewing longer code
- Need more conversation history
- Complex multi-turn discussions
```bash
python utilities/speed_profiles.py quality
```

## Quick Commands

```bash
# Check current context size
echo $NUM_CTX

# Set context size temporarily
export NUM_CTX=8192

# Use speed profiles for easy switching
python utilities/speed_profiles.py balanced
```

Your current 8K setting provides a great balance - you'll notice faster responses while still having enough context for most tasks!